{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing**\n",
    "   - Data loading and preprocessing (e.g., normalization, resizing, augmentation).\n",
    "   - Create visualizations of some images, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "# Cargar CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar muestras\n",
    "num_classes = 10\n",
    "samples_per_class = 10\n",
    "fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(15, 15))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    class_indices = np.where(y_train == i)[0]\n",
    "    random_indices = np.random.choice(class_indices, samples_per_class, replace=False)\n",
    "    for j, idx in enumerate(random_indices):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(x_train[idx])\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Model Architecture**\n",
    "   - Design a CNN architecture suitable for image classification.\n",
    "   - Include convolutional layers, pooling layers, and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here :\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(32,32,3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Training**\n",
    "   - Train the CNN model using appropriate optimization techniques (e.g., stochastic gradient descent, Adam).\n",
    "   - Utilize techniques such as early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                   batch_size=1024,\n",
    "                   epochs=100,\n",
    "                   validation_split=0.2,\n",
    "                   callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Model Evaluation**\n",
    "   - Evaluate the trained model on a separate validation set.\n",
    "   - Compute and report metrics such as accuracy, precision, recall, and F1-score.\n",
    "   - Visualize the confusion matrix to understand model performance across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predicciones\n",
    "predictions = model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Asegurarse de que las clases estén en el rango correcto (0-9 para CIFAR-10)\n",
    "print(\"Rango de predicciones:\", np.min(predictions_classes), \"-\", np.max(predictions_classes))\n",
    "print(\"Rango de valores reales:\", np.min(true_classes), \"-\", np.max(true_classes))\n",
    "\n",
    "# Calcular métricas solo si los rangos son correctos\n",
    "if (0 <= np.min(predictions_classes) <= 9) and (0 <= np.max(predictions_classes) <= 9):\n",
    "    print(\"\\nMatriz de Confusión:\")\n",
    "    print(confusion_matrix(true_classes, predictions_classes))\n",
    "\n",
    "    print(\"\\nMétricas de Evaluación:\")\n",
    "    print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "else:\n",
    "    print(\"Error: Las predicciones no están en el rango esperado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Transfer Learning**\n",
    "    - Evaluate the accuracy of your model on a pre-trained models like ImagNet, VGG16, Inception... (pick one an justify your choice)\n",
    "        - You may find this [link](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) helpful.\n",
    "        - [This](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) is the Pytorch version.\n",
    "    - Perform transfer learning with your chosen pre-trained models i.e., you will probably try a few and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Cargar VGG16 pre-entrenado\n",
    "base_model = VGG16(weights='imagenet', \n",
    "                   include_top=False, \n",
    "                   input_shape=(32, 32, 3))\n",
    "                   #classifier_activation=\"SoftMax\"))\n",
    "\n",
    "# Congelar capas base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Crear modelo de transfer learning\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "transfer_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Entrenar modelo de transfer learning\n",
    "transfer_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_transfer = transfer_model.fit(x_train, \n",
    "                                    y_train,\n",
    "                                    batch_size=128,\n",
    "                                    epochs=50,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluar métricas después del transfer learning\n",
    "predictions = transfer_model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nMétricas después del transfer learning:\")\n",
    "print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "transfer_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_fine_tuning = transfer_model.fit(x_train, \n",
    "                                       y_train,\n",
    "                                       batch_size=128,\n",
    "                                       epochs=30,\n",
    "                                       validation_data=(x_test, y_test),\n",
    "                                       callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluar métricas después del fine-tuning\n",
    "predictions = transfer_model.predict(x_test)\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nMétricas después del fine-tuning:\")\n",
    "print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predicciones\n",
    "predictions = transfer_model.predict(x_test)  # Nota: cambiado de base_model a transfer_model\n",
    "predictions_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Asegurarse de que las clases estén en el rango correcto (0-9 para CIFAR-10)\n",
    "print(\"Rango de predicciones:\", np.min(predictions_classes), \"-\", np.max(predictions_classes))\n",
    "print(\"Rango de valores reales:\", np.min(true_classes), \"-\", np.max(true_classes))\n",
    "\n",
    "# Calcular métricas solo si los rangos son correctos\n",
    "if (0 <= np.min(predictions_classes) <= 9) and (0 <= np.max(predictions_classes) <= 9):\n",
    "    print(\"\\nMatriz de Confusión:\")\n",
    "    print(confusion_matrix(true_classes, predictions_classes))\n",
    "\n",
    "    print(\"\\nMétricas de Evaluación:\")\n",
    "    print(f\"F1-Score: {f1_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Precisión: {precision_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(true_classes, predictions_classes, average='weighted'):.4f}\")\n",
    "else:\n",
    "    print(\"Error: Las predicciones no están en el rango esperado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Code Quality**\n",
    "   - Well-structured and commented code.\n",
    "   - Proper documentation of functions and processes.\n",
    "   - Efficient use of libraries and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Report**\n",
    "   - Write a concise report detailing the approach taken, including:\n",
    "     - Description of the chosen CNN architecture.\n",
    "     - Explanation of preprocessing steps.\n",
    "     - Details of the training process (e.g., learning rate, batch size, number of epochs).\n",
    "     - Results and analysis of models performance.\n",
    "     - What is your best model. Why?\n",
    "     - Insights gained from the experimentation process.\n",
    "   - Include visualizations and diagrams where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Image Classification Report\n",
    "\n",
    "## 1. CNN Architecture\n",
    "Our implementation used two main approaches:\n",
    "\n",
    "### Custom CNN Architecture\n",
    "The custom CNN model follows a VGG-style architecture with:\n",
    "- 3 convolutional blocks with increasing filters (64 → 128 → 256)\n",
    "- Each block contains:\n",
    "  - Two Conv2D layers with 3x3 kernels\n",
    "  - BatchNormalization for training stability\n",
    "  - MaxPooling2D for dimensionality reduction\n",
    "  - Dropout for regularization (increasing from 0.3 to 0.5)\n",
    "- Final dense layers:\n",
    "  - 512 units with ReLU activation\n",
    "  - 10 units with softmax for classification\n",
    "\n",
    "### Transfer Learning Model\n",
    "We utilized VGG16 pretrained on ImageNet:\n",
    "- Base VGG16 model with frozen weights\n",
    "- Custom top layers:\n",
    "  - Global Average Pooling\n",
    "  - Dense layer (512 units)\n",
    "  - Dropout (0.5)\n",
    "  - Output layer (10 units)\n",
    "\n",
    "## 2. Preprocessing Steps\n",
    "- Image normalization (scaling pixel values to 0-1)\n",
    "- Data augmentation using ImageDataGenerator:\n",
    "  - Rotation (±15°)\n",
    "  - Width/height shifts (±10%)\n",
    "  - Horizontal flips\n",
    "  - Zoom range (±10%)\n",
    "- One-hot encoding of labels\n",
    "\n",
    "## 3. Training Process\n",
    "### Custom CNN:\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Batch size: 1024\n",
    "- Epochs: 100 (with early stopping)\n",
    "- Validation split: 20%\n",
    "\n",
    "### Transfer Learning:\n",
    "- Initial training:\n",
    "  - Learning rate: 0.001\n",
    "  - Batch size: 128\n",
    "  - Epochs: 50\n",
    "- Fine-tuning:\n",
    "  - Learning rate: 0.0001\n",
    "  - Last 4 layers unfrozen\n",
    "  - Epochs: 30\n",
    "\n",
    "## 4. Model Performance\n",
    "### Custom CNN Results:\n",
    "- Accuracy: ~75%\n",
    "- F1-Score: 0.74\n",
    "- Precision: 0.75\n",
    "- Recall: 0.74\n",
    "\n",
    "### Transfer Learning Results:\n",
    "- Accuracy: ~82%\n",
    "- F1-Score: 0.81\n",
    "- Precision: 0.82\n",
    "- Recall: 0.81\n",
    "\n",
    "## 5. Best Model Analysis\n",
    "The transfer learning approach with VGG16 proved to be the superior model for several reasons:\n",
    "- Higher overall accuracy and F1-score\n",
    "- Better generalization on test data\n",
    "- Faster convergence during training\n",
    "- Leveraged pre-learned features from ImageNet\n",
    "\n",
    "## 6. Key Insights\n",
    "1. Transfer learning significantly outperformed the custom CNN, demonstrating the value of pretrained models\n",
    "2. Data augmentation was crucial for preventing overfitting\n",
    "3. Batch normalization improved training stability\n",
    "4. Progressive dropout rates helped manage overfitting in deeper layers\n",
    "5. Fine-tuning the last few layers of VGG16 provided additional performance improvements\n",
    "\n",
    "The project demonstrates the effectiveness of transfer learning for image classification tasks, especially when working with limited computational resources and relatively small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Model deployment**\n",
    "     - Pick the best model \n",
    "     - Build an app using Flask - Can you host somewhere other than your laptop? **+5 Bonus points if you use [Tensorflow Serving](https://www.tensorflow.org/tfx/guide/serving)**\n",
    "     - User should be able to upload one or multiples images get predictions including probabilities for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el mejor modelo (el que tenga mejor rendimiento entre el base y transfer learning)\n",
    "model.save('best_model.keras')\n",
    "\n",
    "# Crear aplicación Flask\n",
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = load_model('best_model.keras')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    file = request.files['image']\n",
    "    img = Image.open(file)\n",
    "    img = img.resize((32, 32))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    pred = model.predict(img_array)\n",
    "    class_idx = np.argmax(pred[0])\n",
    "    confidence = float(pred[0][class_idx])\n",
    "    \n",
    "    return jsonify({\n",
    "        'class': int(class_idx),\n",
    "        'confidence': confidence\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
